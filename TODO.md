# TODO

- **TODAY**:
  - [x] Make crawler pip installable.
    - [x] add really good documentation so that it can be used as a library and easily integrated into the server via llm.
    - [x] remove all docling stuff.
    - [x] remove all the default configs
    - [ ] good logging set in the initial config for the crawler.
  - Simplify:
    - [ ] Server api
      - [ ] Easy to add documents to existing collections
      - [ ] Easy to create new collections
    - [ ] Crawler api
      - [ ] Easy to create custom crawlers for all sorts of different file types and metadata schemas.
        - [ ] This includes establishing pre-processed documents.
  - [ ] Remove auth from backend
  - [ ] Setup fake auth from upload-ui for testing
    - [ ] Hardcode username and password for testing both in milvus and on UI.
    - [ ] Make a search bar to do quick searches on milvus.
- [ ] Update RAG pipeline for BD docs too.
- [ ] Load real data into milvus
- [ ] Design local-minified version
  - [ ] single binary (ish) that runs, indexes the directories you tell it, then you can chat with it.
  - [ ] files and indexes stored .rrccp